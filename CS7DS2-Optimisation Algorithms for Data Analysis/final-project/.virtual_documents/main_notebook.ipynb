





from src.data_preprocessing import load_data, clean_data, split_data
from src.model import build_model
from src.training import train_model
from src.evaluation import evaluate_model
from src.visualization import plot_performance

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, classification_report
from tensorflow.keras.utils import to_categorical  # 更新导入路径


try:
    import keras
    print("Keras is installed and ready to use.")
except ImportError:
    print("Keras is not installed.")





file_path = 'data/dataset.csv'
data = load_data(file_path)
# data = clean_data(data)
# X_train, X_test, y_train, y_test = split_data(data)





# cols_to_convert = ['Channel', 'Region']
# for col in cols_to_convert:
#     data[col] = data[col].astype('object')
data.info()


data.head(10)


data = clean_data(data)


def split_data(data, target_column, categorical_columns):
    # 检查目标列和分类列是否存在于数据中
    missing_cols = [col for col in [target_column] + categorical_columns if col not in data.columns]
    if missing_cols:
        raise ValueError(f"Columns {missing_cols} are missing in the DataFrame")

    label_encoders = {}
    for col in categorical_columns:
        if col != target_column:
            label_encoder = LabelEncoder()
            data[col] = label_encoder.fit_transform(data[col])
            label_encoders[col] = label_encoder
    
    # 分离特征和目标变量
    X = data.drop(target_column, axis=1)
    y = data[target_column]

    # 划分数据集为训练集和测试集
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
    return X_train, X_test, y_train, y_test, label_encoders


target_column = 'Region'  # predict Region
categorical_columns = ['Channel']   # 假设只有Channel是分类特征需要编码
X_train, X_test, y_train, y_test, label_encoders = split_data(data, target_column, categorical_columns)


X_train.head(1) , X_test.head(1), y_train.head(1), y_test.head(1), label_encoders





# 获取输入的维度（特征数量）
input_shape = X_train.shape[1]

# 确定类别的数量
num_classes = y_train.nunique()

# 因为我们正在做分类任务，所以我们需要对标签进行独热编码
y_train_encoded = to_categorical(y_train)
y_test_encoded = to_categorical(y_test)

# 建立模型
model = build_mlp_model(input_shape, num_classes=4)

# 检查模型的结构（适用于Keras模型）
model.summary()





epochs = 100  # epoch
batch_size = 32  # batch size
model, history = train_model(model, X_train, y_train_encoded, X_test, y_test_encoded, epochs, batch_size)





# 调用绘图函数
plot_loss(history)  # 传入训练历史记录对象
plot_accuracy(history)





# 评估模型
evaluate_model(model, X_test, y_test_encoded)

# 如果你有多个评估指标，可以这样打印每个指标
# print(f"Accuracy on test set: {accuracy_score(y_test, model.predict(X_test))}")





# 可视化性能指标
plot_performance(performance_data)


# 混淆矩阵的可视化
plot_confusion_matrix(confusion_matrix, class_names)








# 首先是模型调整一：增加新的隐藏层
def build_mlp_model_v1(input_shape, num_classes):
    model = Sequential()
    model.add(Dense(128, input_shape=(input_shape,), activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(128, activation='relu'))  # New Hidden Layers
    model.add(Dropout(0.5))
    model.add(Dense(64, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(num_classes, activation='softmax'))
    model.compile(optimizer='adam',
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])
    return model

# 获取输入的维度（特征数量）
input_shape = X_train.shape[1]

# 确定类别的数量
num_classes = y_train.nunique()

# 因为我们正在做分类任务，所以我们需要对标签进行独热编码
y_train_encoded = to_categorical(y_train)
y_test_encoded = to_categorical(y_test)

# 建立模型
model = build_mlp_model_v1(input_shape, num_classes=4)

# 检查模型的结构（适用于Keras模型）
model.summary()


epochs = 100  # epoch
batch_size = 32  # batch size
model, history = train_model(model, X_train, y_train_encoded, X_test, y_test_encoded, epochs, batch_size)


# 调用绘图函数
plot_loss(history)  # 传入训练历史记录对象
plot_accuracy(history)
# 可视化性能指标
plot_performance(performance_data)
# 混淆矩阵的可视化
plot_confusion_matrix(confusion_matrix, class_names)


# 首先是模型调整二：增加或减少现有隐藏层中的神经元数目
def build_mlp_model_v2(input_shape, num_classes):
    model = Sequential()
    model.add(Dense(256, input_shape=(input_shape,), activation='relu'))  # 增加到256个神经元
    model.add(Dropout(0.5))
    model.add(Dense(64, activation='relu'))  # 这里也可以调整，例如尝试减少神经元数目
    model.add(Dropout(0.5))
    model.add(Dense(num_classes, activation='softmax'))
    model.compile(optimizer='adam',
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])
    return model


# 获取输入的维度（特征数量）
input_shape = X_train.shape[1]

# 确定类别的数量
num_classes = y_train.nunique()

# 因为我们正在做分类任务，所以我们需要对标签进行独热编码
y_train_encoded = to_categorical(y_train)
y_test_encoded = to_categorical(y_test)

# 建立模型
model = build_mlp_model_v2(input_shape, num_classes=4)

# 检查模型的结构（适用于Keras模型）
model.summary()


epochs = 100  # epoch
batch_size = 32  # batch size
model, history = train_model(model, X_train, y_train_encoded, X_test, y_test_encoded, epochs, batch_size)


# 调用绘图函数
plot_loss(history)  # 传入训练历史记录对象
plot_accuracy(history)
# 可视化性能指标
plot_performance(performance_data)
# 混淆矩阵的可视化
plot_confusion_matrix(confusion_matrix, class_names)


# 首先是模型调整三：同时增加隐藏层的数量和调整神经元数目
def build_mlp_model_v3(input_shape, num_classes):
    model = Sequential()
    model.add(Dense(256, input_shape=(input_shape,), activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(128, activation='relu'))  # 新增的隐藏层
    model.add(Dropout(0.5))
    model.add(Dense(128, activation='relu'))  # 维持较高的神经元数目
    model.add(Dropout(0.5))
    model.add(Dense(num_classes, activation='softmax'))
    model.compile(optimizer='adam',
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])
    return model


# 获取输入的维度（特征数量）
input_shape = X_train.shape[1]

# 确定类别的数量
num_classes = y_train.nunique()

# 因为我们正在做分类任务，所以我们需要对标签进行独热编码
y_train_encoded = to_categorical(y_train)
y_test_encoded = to_categorical(y_test)

# 建立模型
model = build_mlp_model_v3(input_shape, num_classes=4)

# 检查模型的结构（适用于Keras模型）
model.summary()


epochs = 100  # epoch
batch_size = 32  # batch size
model, history = train_model(model, X_train, y_train_encoded, X_test, y_test_encoded, epochs, batch_size)


# 调用绘图函数
plot_loss(history)  # 传入训练历史记录对象
plot_accuracy(history)
# 可视化性能指标
plot_performance(performance_data)
# 混淆矩阵的可视化
plot_confusion_matrix(confusion_matrix, class_names)





# 1.优化器调整之一：学习率大小
# 设置优化器，这里使用Adam，并设置一个初始学习率
from tensorflow.keras.optimizers import Adam
def build_mlp_model_v3(input_shape, num_classes):
    model = Sequential()
    model.add(Dense(256, input_shape=(input_shape,), activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(128, activation='relu'))  # 新增的隐藏层
    model.add(Dropout(0.5))
    model.add(Dense(128, activation='relu'))  # 维持较高的神经元数目
    model.add(Dropout(0.5))
    model.add(Dense(num_classes, activation='softmax'))
    optimizer = Adam(learning_rate=0.001)
    model.compile(optimizer=optimizer,                       # 这里设置一个初始化的学习率【由原来的adam改为 Adam(learning_rate=0.001)】
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])
    return model


from tensorflow.keras.callbacks import LearningRateScheduler, ReduceLROnPlateau

# 学习率调度器函数
def lr_schedule(epoch, lr):
    if epoch < 10:
        return lr
    else:
        return lr * 0.9  # 每个epoch学习率衰减10%

# 使用 ReduceLROnPlateau 自动降低学习率
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001)  # 为模型训练做准备


def train_model(model, X_train, y_train_encoded, X_val, y_val_encoded, epochs, batch_size):
    # 可以在这里添加一个模型检查点，以保存训练过程中表现最好的模型
    # （可选）保存模型
    # checkpoint = ModelCheckpoint('results/best_model.h5', monitor='val_loss', save_best_only=True)
    checkpoint = ModelCheckpoint('results/best_model_1.keras', monitor='val_loss', save_best_only=True)         # 最新版本
    # 训练模型
    history = model.fit(
        X_train, y_train_encoded,
        validation_data=(X_test, y_test_encoded),
        epochs=100,
        batch_size=64,
        callbacks=[LearningRateScheduler(lr_schedule), reduce_lr],  # 添加学习率调整
        verbose=2
    )
    return model, history


# 获取输入的维度（特征数量）
input_shape = X_train.shape[1]

# 确定类别的数量
num_classes = y_train.nunique()

# 因为我们正在做分类任务，所以我们需要对标签进行独热编码
y_train_encoded = to_categorical(y_train)
y_test_encoded = to_categorical(y_test)

# 建立模型
model = build_mlp_model_v3(input_shape, num_classes=4)

# 检查模型的结构（适用于Keras模型）
model.summary()


# epochs = 100  # epoch
# batch_size = 32  # batch size
model, history = train_model(model, X_train, y_train_encoded, X_test, y_test_encoded, epochs, batch_size)


# 调用绘图函数
plot_loss(history)  # 传入训练历史记录对象
plot_accuracy(history)
# evaluate_model(model, X_test, y_test_encoded)

# 可视化性能指标
plot_performance(performance_data)
# 混淆矩阵的可视化
plot_confusion_matrix(confusion_matrix, class_names)


# 2.优化器调整之一：SGD大小【优化器调整，是在model.py文件中，模型编译这里优化的】
# 和学习率一样，使用相同的tarining脚本
from tensorflow.keras.optimizers import SGD

def build_mlp_model_v3(input_shape, num_classes):
    model = Sequential()
    model.add(Dense(256, input_shape=(input_shape,), activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(128, activation='relu'))  # 新增的隐藏层
    model.add(Dropout(0.5))
    model.add(Dense(128, activation='relu'))  # 维持较高的神经元数目
    model.add(Dropout(0.5))
    model.add(Dense(num_classes, activation='softmax'))
    optimizer_sgd = SGD(learning_rate=0.01, momentum=0.9)         # 使用具有动量的SGD优化器
    model.compile(optimizer=optimizer_sgd,                       # 这里设置一个初始化的学习率【由原来的adam改为optimizer_sgd】
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])
    return model


# 获取输入的维度（特征数量）
input_shape = X_train.shape[1]

# 确定类别的数量
num_classes = y_train.nunique()

# 因为我们正在做分类任务，所以我们需要对标签进行独热编码
y_train_encoded = to_categorical(y_train)
y_test_encoded = to_categorical(y_test)

# 建立模型
model = build_mlp_model_v3(input_shape, num_classes=4)

# 检查模型的结构（适用于Keras模型）
model.summary()


model, history = train_model(model, X_train, y_train_encoded, X_test, y_test_encoded, epochs, batch_size)


# 调用绘图函数
plot_loss(history)  # 传入训练历史记录对象
plot_accuracy(history)
# evaluate_model(model, X_test, y_test_encoded)

# 可视化性能指标
plot_performance(performance_data)
# 混淆矩阵的可视化
plot_confusion_matrix(confusion_matrix, class_names)


# 2.优化器调整之一：RMSprop优化器
# 和学习率一样，使用相同的tarining脚本
from tensorflow.keras.optimizers import RMSprop

def build_mlp_model_v3(input_shape, num_classes):
    model = Sequential()
    model.add(Dense(256, input_shape=(input_shape,), activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(128, activation='relu'))  # 新增的隐藏层
    model.add(Dropout(0.5))
    model.add(Dense(128, activation='relu'))  # 维持较高的神经元数目
    model.add(Dropout(0.5))
    model.add(Dense(num_classes, activation='softmax'))
    optimizer_rmsprop = RMSprop(learning_rate=0.001)         # 使用具有动量的RMSprop优化器
    model.compile(optimizer=optimizer_rmsprop,                       # 这里设置一个初始化的学习率【由原来的adam改为optimizer_sgd】
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])
    return model


# 获取输入的维度（特征数量）
input_shape = X_train.shape[1]

# 确定类别的数量
num_classes = y_train.nunique()

# 因为我们正在做分类任务，所以我们需要对标签进行独热编码
y_train_encoded = to_categorical(y_train)
y_test_encoded = to_categorical(y_test)

# 建立模型
model = build_mlp_model_v3(input_shape, num_classes=4)

# 检查模型的结构（适用于Keras模型）
model.summary()


model, history = train_model(model, X_train, y_train_encoded, X_test, y_test_encoded, epochs, batch_size)


# 调用绘图函数
plot_loss(history)  # 传入训练历史记录对象
plot_accuracy(history)
# evaluate_model(model, X_test, y_test_encoded)

# 可视化性能指标
plot_performance(performance_data)
# 混淆矩阵的可视化
plot_confusion_matrix(confusion_matrix, class_names)





from tensorflow.keras.optimizers import SGD

def build_mlp_model_v3(input_shape, num_classes):
    model = Sequential()
    model.add(Dense(256, input_shape=(input_shape,), activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(128, activation='relu'))  # 新增的隐藏层
    model.add(Dropout(0.5))
    model.add(Dense(128, activation='relu'))  # 维持较高的神经元数目
    model.add(Dropout(0.5))
    model.add(Dense(num_classes, activation='softmax'))
    optimizer_sgd = SGD(learning_rate=0.01, momentum=0.9)         # 使用具有动量的SGD优化器
    model.compile(optimizer=optimizer_sgd,                       # 这里设置一个初始化的学习率【由原来的adam改为optimizer_sgd】
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])
    return model


from tensorflow.keras.callbacks import LearningRateScheduler, ReduceLROnPlateau

# 设置不同的批量大小进行实验
batch_sizes = [16, 32, 64, 128]

for batch_size in batch_sizes:
    print(f"Training with batch size: {batch_size}")
    # 重新构建模型（如果需要保持模型结构不变）
    # 获取输入的维度（特征数量）
    input_shape = X_train.shape[1]
    
    # 确定类别的数量
    num_classes = y_train.nunique()
    
    # 因为我们正在做分类任务，所以我们需要对标签进行独热编码
    y_train_encoded = to_categorical(y_train)
    y_test_encoded = to_categorical(y_test)
    
    # 建立模型
    model = build_mlp_model_v3(input_shape, num_classes=4)

    # 检查模型的结构（适用于Keras模型）
    model.summary()
    # model = build_mlp_model_v3(input_shape=X_train.shape[1], num_classes=num_classes)

    # # 编译模型
    # model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    
    # 设置学习率调整策略
    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001)
    
    # 训练模型
    history = model.fit(
        X_train, y_train_encoded,
        epochs=100,  # 或根据需要调整epochs的数量
        batch_size=batch_size,
        validation_data=(X_test, y_test_encoded),
        callbacks=[reduce_lr],
        verbose=2
    )
    model, history = train_model(model, X_train, y_train_encoded, X_test, y_test_encoded, epochs, batch_size)
    # 调用绘图函数
    plot_loss(history)  # 传入训练历史记录对象
    plot_accuracy(history)
    # evaluate_model(model, X_test, y_test_encoded)
    # 可以在这里评估模型或保存模型结果
    # evaluate_model(model, X_test, y_test_encoded)
    # model.save(f'results/model_batch_size_{batch_size}.h5')



from tensorflow.keras.optimizers import SGD

def build_small_batch_sgd_model(input_shape, num_classes):
    model = Sequential()
    model.add(Dense(256, input_shape=(input_shape,), activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(128, activation='relu'))                                     # 新增的隐藏层
    model.add(Dropout(0.5))
    model.add(Dense(128, activation='relu'))                                     # 维持较高的神经元数目
    model.add(Dropout(0.5))
    model.add(Dense(num_classes, activation='softmax'))
    optimizer_sgd = SGD(learning_rate=0.001, momentum=0.9, clipnorm=1.0)         # 使用具有动量的SGD优化器[减小学习率]  [使用梯度裁剪：在优化器中使用梯度裁剪。]
    model.compile(optimizer=optimizer_sgd,                                          # 这里设置一个初始化的学习率【由原来的adam改为optimizer_sgd】
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])
    return model


# 训练模型，使用小批量大小
batch_size = 1  # 设置批量大小为1
model = build_small_batch_sgd_model(input_shape=X_train.shape[1], num_classes=4)
model.summary()
history = model.fit(X_train, y_train_encoded, epochs=100, batch_size=batch_size,
                    validation_data=(X_test, y_test_encoded), verbose=2)
model, history = train_model(model, X_train, y_train_encoded, X_test, y_test_encoded, epochs, batch_size)
# 调用绘图函数
plot_loss(history)  # 传入训练历史记录对象
plot_accuracy(history)





# 确认数据维度是否正确：
print("Training set shape:", X_train.shape, y_train_encoded.shape)
print("Test set shape:", X_test.shape, y_test_encoded.shape)


model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])   # 检查模型是否正确编译：


history = model.fit(
    X_train, y_train_encoded,
    epochs=100,
    batch_size=1,
    validation_data=(X_test, y_test_encoded),
    verbose=1  # 设置为1以便查看每个epoch的输出
)





# 训练模型，使用小批量大小
batch_size = 1  # 设置批量大小为1
model = build_small_batch_sgd_model(input_shape=X_train.shape[1], num_classes=4)
model.summary()
history = model.fit(X_train, y_train_encoded, epochs=100, batch_size=batch_size,
                    validation_data=(X_test, y_test_encoded), verbose=2)
model, history = train_model(model, X_train, y_train_encoded, X_test, y_test_encoded, epochs, batch_size)
# 调用绘图函数
plot_loss(history)  # 传入训练历史记录对象
plot_accuracy(history)



